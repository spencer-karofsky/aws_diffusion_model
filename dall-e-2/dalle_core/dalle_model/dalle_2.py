"""
dalle_2: Implements DALL·E 2 architecture, based on the paper.

Description:
    - DALL·E 2 Overview:
        - DALL·E 2 Accepts CLIP text embeddings as input, which encodes each text prompt/caption as a 512-D vector.
        - Consists of two main components:
            1. A prior generates the CLIP image embeddings.
                - These image embeddings are generated given input CLIP text embeddings.
                - The DALL·E 2 paper uses diffusion for the prior (model).
                - Input: CLIP text embeddings ([number of prompts, embedding dimensionality] = [B, 512])
                - Output: CLIP image embeddings ([number of images, embedding dimensionality] = [B, 512])
            2. A decoder generates the actual images.
                - These images are generated given the CLIP image embeddings generated by the decoder.
                    - The CLIP image embeddings are conditioned on the CLIP text embeddings, which are conditioned on the text captions.
                    - This means the CLIP image embeddings contain the meaning of the text captions, so the generated images will correspond to the captions.
                - Input: CLIP image embeddings ([number of images, embedding dimensionality] = [B, 512])
                - Output: Generated images ([number of images, image pixel height, image pixel width] = [B, H, W])
                    - During Inference, we will only generate one or a few images.
                - The decoder is actually two models:
                    1. A DDIM that upsamples from 64^2->256^2
                    2. A DDIM that upsamples from 256^2->1024^2, rendering the final image

Usage:
    from dalle_2 import DALLE2
    dalle = DALLE2(...)

    # Forward Pass
    dalle_2.forward(...)

Classes:
    - DALLE2: defines and runs inference on the DALL·E 2 architecture

References:
    - DALL·E 2 Paper: https://cdn.openai.com/papers/dall-e-2.pdf
    - My DALL·E 2 Notes: https://github.com/spencer-karofsky/aws_diffusion_model/blob/main/dall-e-2/research_notes/DALL-E-2%202022.pdf or /dall-e-2/research_notes/DALL-E-2 2022.pdf

Author:
    - Spencer Karofsky (https://github.com/spencer-karofsky)
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import List

from tqdm import tqdm

from dalle_core.prior.dalle_2_prior import DiffusionPrior
from dalle_core.decoder.dalle_2_decoder import Decoder
from dalle_core.clip.clip_embedder import CLIPEmbedder


from data.data_scripts.dataset_utils import BaseDataset


class DALLE2(nn.Module):
    def __init__(
        self,
        dataset: BaseDataset,
        optimizer: torch.optim.Optimizer,
        batch_size: int = 32,
        num_epochs: int = 10,
        device: str = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
    ):
        """
        Initializes DALL·E 2 architecture.
        Args:
            dataset: midjourney-style (image, caption) dataset
            optimizer: optimizer to update U-Net
            clip_embedder: pre-trained CLIP encoder
            batch_size: batch size
            num_epochs: number of training epochs
            device: training device (ideally uses CUDA if avaliable)
        """
        super().__init__()

        # Later add support for different hyperparameters, if needed
        self.prior = DiffusionPrior()
        self.decoder = Decoder()
        
        self.dataset = dataset
        self.optimizer = optimizer
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.device = device

        self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)

        self.clip_embedder = CLIPEmbedder()
    
    def forward(self, captions: List[str], deterministic: bool = False) -> torch.Tensor:
        """
        Inference: captions → generated images.
        Args:
            captions: list of text prompts (length B)
            deterministic: whether to use deterministic DDIM sampling
        Returns:
            images: [B, 3, H, W]
        """
        B = len(captions)

        # Convert text to CLIP embeddings
        text_embed = self.clip_embedder.encode_text(captions).to(self.device)  # [B, 512]

        # 2. Generate noised image embedding 
        t = torch.randint(0, 1000, (B,), device=self.device)
        t_embed = self.prior.time_embedding(t)

        noised_img_embed = torch.randn(B, self.prior.dim, device=self.device)
        clip_img_embed = self.prior(text_embed, t_embed, noised_img_embed)

        # 3. Decode the CLIP image embeddings to final, cleaned images
        image = self.decoder(clip_img_embed, deterministic=deterministic)
        return image