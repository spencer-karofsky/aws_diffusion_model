"""
dalle_2_decoder: implements the diffusion-powered decoder, introduced in the DALL·E 2 paper: https://cdn.openai.com/papers/dall-e-2.pdf

Description:
    - There are two components/models in DALL·E 2 that work together:
        1. A prior generates CLIP image embeddings given a text caption/prompt.
        2. A decoder generates an image conditioned on the image embedding generated by the prior.
    - This Python file implements the decoder.
    - Decoder Architecture: 2 DDIM Models
        1. First upsampling model: 64x64 → 256x256
        - Inputs:
            - A 64x64 image sampled from Gaussian noise (N(0, I))
            - A CLIP image embedding of shape [B, 512]
                - The CLIP image embedding is projected using a Linear layer to match the dimensionality of the time embedding: [B, 512]
                - During training, the model predicts a slightly blurred version of the target 256x256 image for improved robustness.
        2. Second model upsamples from 256x256 -> 1024x1024.
            - Uses BSR Degredation for more diverse corruption:
                1. Blur the original image with a Gaussian filter.
                2. Subsample/downsample, reducing the resolution.
                3. Reintroduce Gaussian noise.
                4. Upsample the image using diffusion (U-Net)
    - The DALL·E 2 Decoder Outputs a 1024x1024 denoised image using these two DDIMs.

Usage:
    from dalle_2_decoder import Decoder
    decoder = Decoder(...)
    decoder.forward(...)

Classes:
    - Decoder: implements the DALL·E 2 decoder, as explained above in detail.

References:
    - DALL·E 2 Paper: https://cdn.openai.com/papers/dall-e-2.pdf
    - My DALL·E 2 Notes: https://github.com/spencer-karofsky/aws_diffusion_model/blob/main/dall-e-2/research_notes/DALL-E-2%202022.pdf or /dall-e-2/research_notes/DALL-E-2 2022.pdf

Author:
    - Spencer Karofsky (https://github.com/spencer-karofsky)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

from dalle_core.unet.unet import UNetDenoiser
from dalle_core.diffusion_models.noise_scheduler import NoiseScheduler
from dalle_core.diffusion_models.ddim_forward_diffusion import ForwardDiffuser
from dalle_core.clip.clip_embedder import CLIPEmbedder
from dalle_core.diffusion_models.ddim_sampler import DDIMSampler

class Decoder(nn.Module):
    def __init__(
            self,
            base_channels: int = 128,
            time_emb_dim: int = 512,
            timesteps: int = 50
        ):
        """Implements the Decoder described in the DALL·E 2 and above.
        Args:
            base_channels: number of channels in the first convolutional layer of the U-Net
            time_emb_dim: the time embedding dimensionality
            timesteps: the number of timesteps
        """
        super().__init__()

        # Initialize diffusion utilities
        self.scheduler = NoiseScheduler(timesteps=timesteps)
        self.diffuser = ForwardDiffuser(self.scheduler)
        self.timesteps = timesteps

        # Initialize 64 -> 256 upsampler
        self.first_unet = UNetDenoiser(
            base_channels=base_channels,
            time_emb_dim=time_emb_dim,
            image_size=256
        )

        # Initialize 256 -> 1024 upsampler
        self.second_unet = UNetDenoiser(
            base_channels=base_channels,
            time_emb_dim=time_emb_dim,
            image_size=1024
        )

        self.first_upsampler = DDIMSampler(
            self.first_unet,
            self.scheduler
        )

        self.second_upsampler = DDIMSampler(
            self.second_unet,
            self.scheduler
        )

        # CLIP embedder
        self.clip_embedder = CLIPEmbedder()

    def forward(
            self,
            clip_image_embed: torch.Tensor,
            deterministic: bool = False
        ) -> torch.Tensor:
        """Runs the two-stage diffusion decoder
        Args:
            clip_image_embed: tensor of CLIP image embeddings
            deterministic: if enabled, the sampling becomes deterministic (the same output given the same input)
        Returns:
            clean_image: the cleaned (final/fully-generated) image
        """
        B = clip_image_embed.shape[0]
        device = clip_image_embed.device

        # Sample pure noise images (64x64)
        x = torch.randn(B, 3, 64, 64, device=device)

        # Use bilinear interpolation to upsample to 256x256
        x = F.interpolate(
            x,
            size=(256, 256),
            mode='bilinear',
            align_corners=False
        )

        for t in reversed(range(self.timesteps)):
            t_tensor = torch.full((B,), t, dtype=torch.long, device=device)
            x = self.first_upsampler(x, t_tensor, clip_image_embed, deterministic=deterministic)

        # Use bilinear interpolation to further upsample to 1024x1024
        x = F.interpolate(
            x,
            size=(1024, 1024),
            mode='bilinear',
            align_corners=False
        )

        for t in reversed(range(self.timesteps)):
            t_tensor = torch.full((B,), t, dtype=torch.long, device=device)
            x = self.second_upsampler(x, t_tensor, clip_image_embed, deterministic=deterministic)

        return x