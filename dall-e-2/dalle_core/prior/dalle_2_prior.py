"""
dalle_2_prior: implements the diffusion prior integral to the DALL路E 2 paper: https://cdn.openai.com/papers/dall-e-2.pdf

Description and Purpose:
    - There are two components/models in DALL路E 2 that work together:
        1. A prior generates CLIP image embeddings given a text caption/prompt.
        2. A decoder generates an image conditioned on the image embedding generated by the prior.
    - This file implements the prior.
    - Two options for the prior: an autoregressive (AR) model or diffusion-based.
        - The diffusion implementation of the prior achieves better results in the paper, so I am choosing to implement just the diffusion model.
    
    - The diffusion prior trains a decoder-only Transformer with a casual attention mask on a sequence consisting of (in order):
        1. (optional) Encoded text: a [batch size (B), input tokens (padded to 77), dimensionality of the model (512 in the paper)]-Tensor
            - Token-level CLIP embeddings of the text input
        2. CLIP text embedding: a [B, 512]-Tensor that's pooled from the encoded text
            - Caption-level CLIP embedding of the text input
            - Whereas (1.) captures a fine-grained meaning of the input caption, (2.) is a more concise/global representation of the input.
                - Analogy: (1.) is an entire movie, while (2.) is the trailer.
        3. Embedding for the diffusion timestep: a [B, D]-Tensor that encodes the time step of the diffusion process.
            - The scalar timestep, t, encoded into a high-dimensional vector by a sinusoidal embedding function
            - We pick random time steps for each of the B inputs.
        4. Target output of the diffusion prior: a [B, 512]-Tensor containing the intermediate output of the CLIP image embedding.
            - We are trying to denoise an image, step-by-step.
            - (4) is the intermediate output (slightly less noisy image given the current noisy image).
            - We get this from the forward diffusion process.
        5. (optional) Trainable embedding vector: a [1, 512]-Tensor that represents "nothing".
            - In the absense of a provided input caption, it enables DALL路E 2 to still generate images.
            - Learned by the Neural Network (nn.Parameter(...))

        - The authors only implement 2, 3, and 4, so I will also omit 1 and 5

    - The diffusion prior is explained in the paper, and in my notes: https://github.com/spencer-karofsky/aws_diffusion_model/blob/main/dall-e-2/notes/DALL-E-2%202022.pdf

Usage:
    from dalle_2_prior import DALLE2Prior
    prior = DALLE2Prior()
    # text embeddings, timestep embeddings, and noised image embeddings: defined tensors
    clip_img_emb = prior.forward(txt_emb, timestep_emb, noisy_img_emb)

Class:
    - DiffusionPrior: The first half of DALL路E 2 that generates the CLIP image embedding given the text

Author:
    - Spencer Karofsky (https://github.com/spencer-karofsky)
"""
import torch
import torch.nn as nn
from dalle_core.unet.unet import TimeEmbedding

class DiffusionPrior(nn.Module):
    def __init__(
            self,
            n_dim: int = 512,
            n_layers: int = 8,
            n_attention_heads: int = 8
        ):
        """Initializes the diffusion prior
        Args:
            n_dim: the dimensionality of the CLIP embeddings (the paper uses a CLIP model that uses dim=512)
            n_layers: the number of the stacked Transformer decoder layers (the model depth)
            n_attention_heads: the number of attention heads per self-attention layer
        """
        super().__init__()
        
        self.dim = n_dim

        self.time_embedding = TimeEmbedding(self.dim)

        # Initializes learned parameters for positional embedding, since the Transformer needs positional embedding
        self.positional_embedding = nn.Parameter(torch.randn(1, 3, self.dim))

        # Initialize Transformer Encoder layer to function as decoder-only Transformer, as implemented in the paper
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.dim,
            nhead=n_attention_heads,
            batch_first=True # prevents the Transformer from looking ahead (casual attention)
        )

        # Initialize Transformer Encoder using encoder_layer
        self.transformer = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_layers
        )

        # Initialize network to predict denoised image embedding given the Transformer output
        self.output_network = nn.Sequential(
            nn.LayerNorm(self.dim),
            nn.Linear(self.dim, self.dim)
        )

    def forward(
            self,
            text_embedding: torch.Tensor,
            timestep: torch.Tensor,
            noised_image_embedding: torch.Tensor
        ) -> torch.Tensor:
        """
        Args:
            text_embedding: [B, 512]
            timestep_embedding: [B, 512]
            noised_image_embedding: [B, 512]
        Returns:
            denoised_image_embed: [B, 512]
        """
        # B = text_embedding.size(0)
        # Expand to [B, 1, 512]
        text_embedding = text_embedding.unsqueeze(1)
        timestep_embedding = self.time_embedding(timestep)  # [B, 512]
        timestep_embedding = timestep_embedding.unsqueeze(1)

        noised_image_embedding = noised_image_embedding.unsqueeze(1)

        # Concatenate into sequence: [B, 3, 512]
        transformer_input = torch.cat([
            text_embedding,
            timestep_embedding,
            noised_image_embedding
        ], dim=1)

        # Add learned positional embedding
        transformer_input = transformer_input + self.positional_embedding

        # Causal mask (allow attending to current and past tokens)
        seq_len = transformer_input.size(1)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=transformer_input.device) * float('-inf'), diagonal=1)

        # Transformer forward pass with causal mask
        transformer_output = self.transformer(transformer_input, mask=causal_mask)

        # Predict final denoised CLIP image embedding from last token
        denoised_image_embed = self.output_network(transformer_output[:, -1, :])  # [B, 512]

        return denoised_image_embed