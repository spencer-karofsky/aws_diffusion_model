"""
diffusion_prior: implements the diffusion prior integral to the DALL·E 2 paper: https://cdn.openai.com/papers/dall-e-2.pdf

Description and Purpose:
    - There are two components/models in DALL·E 2 that work together:
        1. A prior generates CLIP image embeddings given a text caption/prompt.
        2. A decoder generates an image conditioned on the image embedding generated by the prior.
    - This file implements the prior.
    - Two options for the prior: an autoregressive (AR) model or diffusion-based.
        - The diffusion implementation of the prior achieves better results in the paper, so I am choosing to implement just the diffusion model.
    
    - The diffusion prior trains a decoder-only Transformer with a casual attention mask on a sequence consisting of (in order):
        1. (optional) Encoded text: a [batch size (B), input tokens (padded to 77), dimensionality of the model (512 in the paper)]-Tensor
            - Token-level CLIP embeddings of the text input
        2. CLIP text embedding: a [B, 512]-Tensor that's pooled from the encoded text
            - Caption-level CLIP embedding of the text input
            - Whereas (1.) captures a fine-grained meaning of the input caption, (2.) is a more concise/global representation of the input.
                - Analogy: (1.) is an entire movie, while (2.) is the trailer.
        3. Embedding for the diffusion timestep: a [B, D]-Tensor that encodes the time step of the diffusion process.
            - The scalar timestep, t, encoded into a high-dimensional vector by a sinusoidal embedding function
            - We pick random time steps for each of the B inputs.
        4. Target output of the diffusion prior: a [B, 512]-Tensor containing the intermediate output of the CLIP image embedding.
            - We are trying to denoise an image, step-by-step.
            - (4) is the intermediate output (slightly less noisy image given the current noisy image).
            - We get this from the forward diffusion process.
        5. (optional) Trainable embedding vector: a [1, 512]-Tensor that represents "nothing".
            - In the absense of a provided input caption, it enables DALL·E 2 to still generate images.
            - Learned by the Neural Network (nn.Parameter(...))

        - The authors only implement 2, 3, and 4, so I will also omit 1 and 5

    - The diffusion prior is explained in the paper, and in my notes: https://github.com/spencer-karofsky/aws_diffusion_model/blob/main/dall-e-2/notes/DALL-E-2%202022.pdf

Usage:
    TODO

Classes:
    TODO

Author:
    - Spencer Karofsky (https://github.com/spencer-karofsky)
"""
import math
import torch
import torch.nn as nn
from typing import List
import torch.nn.functional as F

class DiffusionPrior(nn.Module):
    def __init__(
            self,
            n_dim: int = 512,
            n_layers: int = 8,
            n_attention_heads: int = 8
        ):
        """Initializes the diffusion prior
        Args:
            n_dim: the dimensionality of the CLIP embeddings (the paper uses a CLIP model that uses dim=512)
            n_layers: the number of the stacked Transformer decoder layers (the model depth)
            n_attention_heads: the number of attention heads per self-attention layer
        """
        super().__init__()
        
        self.dim = n_dim

        # Initializes learned parameters for positional embedding, since the Transformer needs positional embedding
        self.positional_embedding = nn.Parameter(torch.randn(1, 3, self.dim))

        # Initialize Transformer Encoder layer to function as decoder-only Transformer, as implemented in the paper
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.dim,
            nhead=n_attention_heads,
            batch_first=True # prevents the Transformer from looking ahead (casual attention)
        )

        # Initialize Transformer Encoder using encoder_layer
        self.transformer = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_layers
        )

        # Initialize network to predict denoised image embedding given the Transformer output
        self.output_network = nn.Sequential(
            nn.LayerNorm(self.dim),
            nn.Linear(self.dim, self.dim)
        )

    def forward(
            self,
            text_embedding: torch.Tensor,
            timestep_embedding: torch.Tensor,
            noised_image_embedding: torch.Tensor
        ) -> torch.Tensor:
        """Generates the CLIP image embedding given a text caption/prompt (CLIP-embedded)
        Args:
            text_embedding: the CLIP text embedding, capturing a global context, of shape [B, 512]
            timestep_embedding: the sinusoidal timestep embedding, of shape [B, 512]
            noised_image_embedding: the CLIP embedding of the noisy (intermediate) target output
        Returns:
            the fully de-noised CLIP image embedding (of the same dimensionality as the CLIP text embedding input)
        """
        B = text_embedding.shape[0]

        # Expand the three tensors so they match the expected dimensions: [B, 1, 512]
        text_embedding = text_embedding.unsqueeze(1) # 1 specifies to add the extra dimension at axis 1
        timestep_embedding = timestep_embedding.unsqueeze(1)
        noised_image_embedding = noised_image_embedding.unsqueeze(1)

        # Concatenate the input sequence, so that we can pass into the transformer
        transformer_input = torch.cat([text_embedding,
                                       timestep_embedding,
                                       noised_image_embedding], axis=1) # outputs shape [B, 3, 512]
        
        # Add (learned) positional context to the Transformer input
        transformer_input = transformer_input + self.positional_embedding

        # Pass through the decoder-only Transformer to update these values by using self-attention
        transformer_output = self.transformer(transformer_input) # Outputs the same tensor shape [B, 3, 512]

        # Get the last token's noised image embedding of the three tokens (we have B sequences of three tokens of 512-dimensionality)
        denoised_image_embed = self.output_network(transformer_output[:, -1, :]) # only passes in the last token

        return denoised_image_embed